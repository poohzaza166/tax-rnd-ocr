{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc453daf3181407fa5e5ea43ace29559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from gpu_llm import GpuLLM, LLMConfig\n",
    "\n",
    "conf = LLMConfig(max_length=1000,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_k=0,\n",
    "                top_p=1,\n",
    "                repetition_penalty=1.0,\n",
    "                early_stopping=False,\n",
    "                # _model = None,\n",
    "                # _stop_regex = None,\n",
    "                # stopping_criteria = \"</s>\",\n",
    "                )\n",
    "\n",
    "\n",
    "open_ai = GpuLLM(config=conf,\n",
    "                    seed=153513,\n",
    "                model_name = \"/lustre/scratch/public/falcon-40b-instruct\",\n",
    "                device=\"auto\",\n",
    "                skip_validation=True,\n",
    "                # stop_msgs=[\"</s>\"],\n",
    "                )\n",
    "\n",
    "open_ai._initialize()\n",
    "\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# open_ai = HuggingFacePipeline.from_model_id(\n",
    "# model_id= \"/lustre/scratch/public/falcon-40b-instruct\",\n",
    "# device=\"auto\",\n",
    "# task=\"text-generation\", model_kwargs={\"temperature\": 0},\n",
    "# # streaming=True\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, initialize_agent,AgentType,Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "\n",
    "# search = GoogleSearchAPIWrapper(google_api_key=GOOGLE_API_KEY,google_cse_id=GOOGLE_CSE_ID)\n",
    "# TODO: add coding tools\n",
    "tools = [\n",
    "    # Tool(\n",
    "    # name=\"Google\",\n",
    "    # func=search.run,\n",
    "    # description=\"Use to perfrom Google search through out the internet\"\n",
    "    # ),\n",
    "    Tool(name=\"get_Total_Income\",\n",
    "        func=lambda x : \"user had earn a total of 1700$\",\n",
    "        description=\"Use to get User Total Income\"\n",
    "        ),\n",
    "    Tool(name=\"get_Income_Source\",\n",
    "        func=lambda x : \"User have a following source of income recently. food, clothes, cars\",\n",
    "        description=\"Use to get User Income Source\"\n",
    "        ),\n",
    "    Tool(name=\"get_Total_Expense\",\n",
    "        func=lambda x : \"user had spend a total of 250$\",\n",
    "        description=\"Use to get User Total Expense\"\n",
    "        ),\n",
    "    Tool(name=\"get_Expense\",\n",
    "        func=lambda x : \"user had spend 10$ by buying food, 40$ for rent, 100$ for clothes, 100$ for cars\",\n",
    "        description=\"Use to get User expense\"\n",
    "        ),\n",
    "    Tool(name=\"get_Income\",\n",
    "        func=lambda x : \"user had earn 100$ by selling food, 600$ by selling clothes, 1000$ by selling cars\",\n",
    "        description=\"Use to get User income\"\n",
    "        ),\n",
    "    \n",
    "        \n",
    "] + load_tools(\n",
    "    ['llm-math'], llm=open_ai,\n",
    "    )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import ZeroShotAgent, AgentExecutor\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "agent_prefix = \"\"\"You are a financial assistance. your job is to answer the user question and Think logicly! You should use tools to help you answer the question.\n",
    "You have access to the following tools:\"\"\" \n",
    "agent_suffix = \"\"\"Begin!\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "\n",
    "# {question_history}\n",
    "\n",
    "tool_names = [tool.name for tool in tools]\n",
    "agent_convhis = ConversationBufferWindowMemory(k=2,memory_key=\"question_history\",output_key='output')\n",
    "agent_prompt = ZeroShotAgent.create_prompt(tools=tools,\n",
    "                                           prefix=agent_prefix, \n",
    "                                           suffix=agent_suffix,\n",
    "                                           input_variables=[\"input\",'agent_scratchpad','question_history'])\n",
    "agent_custom = ZeroShotAgent(llm_chain=LLMChain(llm=open_ai,prompt=agent_prompt),allowed_tools=tool_names,return_intermediate_steps=True\n",
    "                             ,max_iterations=1)\n",
    "agent_commit = AgentExecutor.from_agent_and_tools(agent=agent_custom,tools=tools,\n",
    "                                                verbose=True,\n",
    "                                                  # memory=agent_convhis,\n",
    "                                                  # handle_parsing_errors=True,\n",
    "                                                  return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "You are a financial assistance. your job is to answer the user question and Think logicly! You should use tools to help you answer the question.\n",
      "You have access to the following tools:\n",
      "\n",
      "Google: Use to perfrom Google search through out the internet\n",
      "get_Total_Income: Use to get User Total Income\n",
      "get_Income_Source: Use to get User Income Source\n",
      "get_Total_Expense: Use to get User Total Expense\n",
      "get_Expense: Use to get User expense\n",
      "get_Income: Use to get User income\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Google, get_Total_Income, get_Income_Source, get_Total_Expense, get_Expense, get_Income, Calculator]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "Question: what is my total expense?\n",
      "Thought: I need to get the total expense\n",
      "Action: get_Total_Expense\n",
      "Action Input:\n",
      "Observation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: I need to get the total expense\n",
      "Action: get_Total_Expense\n",
      "Action Input:\n",
      "Observation:\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3muser had spend a total of 250$\u001b[0m\n",
      "Thought:You are a financial assistance. your job is to answer the user question and Think logicly! You should use tools to help you answer the question.\n",
      "You have access to the following tools:\n",
      "\n",
      "Google: Use to perfrom Google search through out the internet\n",
      "get_Total_Income: Use to get User Total Income\n",
      "get_Income_Source: Use to get User Income Source\n",
      "get_Total_Expense: Use to get User Total Expense\n",
      "get_Expense: Use to get User expense\n",
      "get_Income: Use to get User income\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Google, get_Total_Income, get_Income_Source, get_Total_Expense, get_Expense, get_Income, Calculator]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "Question: what is my total expense?\n",
      "Thought: I need to get the total expense\n",
      "Action: get_Total_Expense\n",
      "Action Input:\n",
      "Observation:\n",
      "Observation: user had spend a total of 250$\n",
      "Thought: I now know the final answer\n",
      "Final Answer: 250$<|endoftext|>\n",
      "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 250$\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my total expense?',\n",
       " 'output': '250$',\n",
       " 'intermediate_steps': [(AgentAction(tool='get_Total_Expense', tool_input='Observation:', log='Thought: I need to get the total expense\\nAction: get_Total_Expense\\nAction Input:\\nObservation:'),\n",
       "   'user had spend a total of 250$')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_commit({'input':\"what is my total expense?\"}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import PromptTemplate\n",
    "# prompt = PromptTemplate(\"\"\"You are a financial assistance. your job is to answer the user question.\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = LLMChain(llm=open_ai,prompt=prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnd-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
