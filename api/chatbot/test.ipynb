{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a54134621a340f59515e34ebb8b0e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from gpu_llm import GpuLLM, LLMConfig\n",
    "\n",
    "conf = LLMConfig(max_length=1000,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_k=0,\n",
    "                top_p=1,\n",
    "                repetition_penalty=1.0,\n",
    "                early_stopping=False,\n",
    "                # _model = None,\n",
    "                # _stop_regex = None,\n",
    "                # stopping_criteria = \"</s>\",\n",
    "                )\n",
    "\n",
    "\n",
    "open_ai = GpuLLM(config=conf,\n",
    "                    seed=153513,\n",
    "                # model_name = \"/lustre/scratch/public/una-cybertron-7b-v2-bf16\",\n",
    "                model_name= \"/lustre/scratch/public/falcon-40b-instruct/\",\n",
    "                device=\"auto\",\n",
    "                skip_validation=True,\n",
    "                # stop_msgs=[\"</s>\"],\n",
    "                )\n",
    "\n",
    "open_ai._initialize()\n",
    "\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# open_ai = HuggingFacePipeline.from_model_id(\n",
    "# model_id= \"/lustre/scratch/public/falcon-40b-instruct\",\n",
    "# device=\"auto\",\n",
    "# task=\"text-generation\", model_kwargs={\"temperature\": 0},\n",
    "# # streaming=True\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summerise the intent from the following text:\n",
      "I am the all mighty god of the "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universe and I have come to earth to save humanity from their sins.<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intent': 'I am the all mighty god of the universe',\n",
       " 'text': ' and I have come to earth to save humanity from their sins.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"intent\"],\n",
    "    template=\"\"\"Summerise the intent from the following text:\n",
    "{intent}\"\"\",\n",
    ")\n",
    "chain = LLMChain(llm=open_ai,prompt=prompt_template)\n",
    "chain(inputs={\"intent\": \"I am the all mighty god of the universe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain/chains/llm_math/base.py:57: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Tool\nargs_schema\n  subclass of BaseModel expected (type=type_error.subclass; expected_class=BaseModel)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# search = SerpAPIWrapper()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m llm_math_chain \u001b[39m=\u001b[39m LLMMathChain(llm\u001b[39m=\u001b[39mopen_ai, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m tools\u001b[39m.\u001b[39mappend(\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     Tool\u001b[39m.\u001b[39;49mfrom_function(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m         func\u001b[39m=\u001b[39;49mllm_math_chain\u001b[39m.\u001b[39;49mrun,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m         name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCalculator\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m         description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39museful for when you need to answer questions about math\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m         args_schema\u001b[39m=\u001b[39;49mCalculatorInput,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m         \u001b[39m# coroutine= ... <- you can specify an async method if desired as well\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     ))\n",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain_core/tools.py:570\u001b[0m, in \u001b[0;36mTool.from_function\u001b[0;34m(cls, func, name, description, return_direct, args_schema, coroutine, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m coroutine \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFunction and/or coroutine must be provided\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 570\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    571\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    572\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    573\u001b[0m     coroutine\u001b[39m=\u001b[39;49mcoroutine,\n\u001b[1;32m    574\u001b[0m     description\u001b[39m=\u001b[39;49mdescription,\n\u001b[1;32m    575\u001b[0m     return_direct\u001b[39m=\u001b[39;49mreturn_direct,\n\u001b[1;32m    576\u001b[0m     args_schema\u001b[39m=\u001b[39;49margs_schema,\n\u001b[1;32m    577\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    578\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain_core/tools.py:550\u001b[0m, in \u001b[0;36mTool.__init__\u001b[0;34m(self, name, func, description, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    547\u001b[0m     \u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, func: Optional[Callable], description: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m    548\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    549\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize tool.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m     \u001b[39msuper\u001b[39;49m(Tool, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    551\u001b[0m         name\u001b[39m=\u001b[39;49mname, func\u001b[39m=\u001b[39;49mfunc, description\u001b[39m=\u001b[39;49mdescription, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    552\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain_core/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Tool\nargs_schema\n  subclass of BaseModel expected (type=type_error.subclass; expected_class=BaseModel)"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools, initialize_agent,AgentType,Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from data_query import Get_all_user_transaction, Get_latest_user_transaction\n",
    "\n",
    "search = GoogleSearchAPIWrapper(k=5, google_api_key=\"AIzaSyBSnAYoQ5w7oKE6s1F7j2MWiDcIJS7ZO0c\",google_cse_id=\"b759664f4a733494f\")\n",
    "# TODO: add coding tools\n",
    "def google_search(query):\n",
    "    return load_summarize_chain(open_ai, chain_type=\"map_reduce\").run([search.run(query)])\n",
    "tools = [\n",
    "    Tool(\n",
    "    name=\"Google\",\n",
    "    func=search.run,\n",
    "    description=\"Use to perfrom Google search through out the internet. But you need to summerise the result yourself\"\n",
    "    ),\n",
    "\n",
    "    # Tool(name=\"get_Total_Income\",\n",
    "    #     func=lambda x : \"user had earn a total of 1700$\",\n",
    "    #     description=\"Use to get User Total Income\"\n",
    "    #     ),\n",
    "    # Tool(name=\"get_Income_Source\",\n",
    "    #     func=lambda x : \"User have a following source of income recently. food, clothes, cars\",\n",
    "    #     description=\"Use to get User Income Source\"\n",
    "    #     ),\n",
    "    # Tool(name=\"get_Total_Expense\",\n",
    "    #     func=lambda x : \"user had spend a total of 250$\",\n",
    "    #     description=\"Use to get User Total Expense\"\n",
    "    #     ),\n",
    "    # Tool(name=\"get_Expense\",\n",
    "    #     func=lambda x : \"user had spend 10$ by buying food, 40$ for rent, 100$ for clothes, 100$ for cars\",\n",
    "    #     description=\"Use to get User expense\"\n",
    "    #     ),\n",
    "    # Tool(name=\"get_Income\",\n",
    "    #     func=lambda x : \"user had earn 100$ by selling food, 600$ by selling clothes, 1000$ by selling cars\",\n",
    "    #     description=\"Use to get User income\"\n",
    "    #     ),\n",
    "    \n",
    "        \n",
    "] + load_tools(\n",
    "    ['llm-math'], llm=open_ai,\n",
    "    ) \n",
    "tools.append(Get_all_user_transaction())   \n",
    "tools.append(Get_latest_user_transaction())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import ZeroShotAgent, AgentExecutor\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "agent_prefix = \"\"\"You are a financial assistance. your job is to answer the user question and Think logicly! You should use tools to help you answer the question. You should perform 2 action at max before deciding the final answer to the question. If the tools dose not return good enough answer, you should answer the question yourself.\n",
    "You have access to the following tools:\"\"\" \n",
    "agent_suffix = \"\"\"Begin!\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "\n",
    "# {question_history}\n",
    "\n",
    "tool_names = [tool.name for tool in tools]\n",
    "# agent_convhis = ConversationBufferWindowMemory(k=2,memory_key=\"question_history\",output_key='output')\n",
    "agent_prompt = ZeroShotAgent.create_prompt(tools=tools,\n",
    "                                           prefix=agent_prefix, \n",
    "                                           suffix=agent_suffix,\n",
    "                                           input_variables=[\"input\",'agent_scratchpad','question_history'])\n",
    "\n",
    "# agent_prompt = PromptTemplate(input_variables=[\"input\",'agent_scratchpad','question_history'],template=\"\"\"\"\"\")\n",
    "\n",
    "\n",
    "agent_custom = ZeroShotAgent(llm_chain=LLMChain(llm=open_ai,prompt=agent_prompt),allowed_tools=tool_names,return_intermediate_steps=True\n",
    "                             ,max_iterations=1)\n",
    "agent_commit = AgentExecutor.from_agent_and_tools(agent=agent_custom,tools=tools,\n",
    "                                                verbose=True,\n",
    "                                                  # memory=agent_convhis,\n",
    "                                                  # handle_parsing_errors=True,\n",
    "                                                  return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "You are a financial assistance. your job is to answer the user question and Think logicly! You should use tools to help you answer the question. You should perform 2 action at max before deciding the final answer to the question. If the tools dose not return good enough answer, you should answer the question yourself.\n",
      "You have access to the following tools:\n",
      "\n",
      "Google: Use to perfrom Google search through out the internet. But you need to summerise the result yourself\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "get_user_transaction: Use this tool to get user transaction\n",
      "get_latest_user_transaction: Use this tool to get latest user transaction\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Google, Calculator, get_user_transaction, get_latest_user_transaction]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "Question: give me a history of my transaction\n",
      "Thought: I need to use the get_user_transaction tool to get the user's transaction history\n",
      "Action: get_user_transaction\n",
      "Action Input: user_id\n",
      "Observation:\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use the get_user_transaction tool to get the user's transaction history\n",
      "Action: get_user_transaction\n",
      "Action Input: user_id\n",
      "Observation:\u001b[0m2023-12-07 16:34:32,970 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2023-12-07 16:34:32,972 INFO sqlalchemy.engine.Engine SELECT users.id AS users_id, users.name AS users_name, users.age AS users_age \n",
      "FROM users \n",
      "WHERE users.id = ?\n",
      " LIMIT ? OFFSET ?\n",
      "2023-12-07 16:34:32,973 INFO sqlalchemy.engine.Engine [generated in 0.00058s] ('a8dcc725-2fbc-4544-9eca-845c14b2e93a', 1, 0)\n",
      "2023-12-07 16:34:32,976 INFO sqlalchemy.engine.Engine SELECT income.id AS income_id, income.amount AS income_amount, income.item AS income_item, income.transaction_time AS income_transaction_time, income.user_id AS income_user_id, income.image_path AS income_image_path \n",
      "FROM income \n",
      "WHERE ? = income.user_id\n",
      "2023-12-07 16:34:32,976 INFO sqlalchemy.engine.Engine [generated in 0.00052s] ('a8dcc725-2fbc-4544-9eca-845c14b2e93a',)\n",
      "2023-12-07 16:34:32,979 INFO sqlalchemy.engine.Engine SELECT expenses.id AS expenses_id, expenses.amount AS expenses_amount, expenses.item AS expenses_item, expenses.transaction_time AS expenses_transaction_time, expenses.user_id AS expenses_user_id, expenses.image_path AS expenses_image_path \n",
      "FROM expenses \n",
      "WHERE ? = expenses.user_id\n",
      "2023-12-07 16:34:32,979 INFO sqlalchemy.engine.Engine [generated in 0.00032s] ('a8dcc725-2fbc-4544-9eca-845c14b2e93a',)\n",
      "2023-12-07 16:34:32,980 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3muser earn 1000000.0 on shadow with time 2023-11-24 13:14:08.577375.\n",
      "user earn 1000000.0 on shadow with time 2023-11-24 13:17:24.033903.\n",
      "user earn 34.0 on yrt with time 2023-11-24 20:20:52.805577.\n",
      "user earn 200.0 on selling car with time 2023-12-01 14:30:16.060111.\n",
      "user spent 34234 on fried rice with time 2023-11-24 13:19:58.288379.\n",
      "user spent 324 on yes with time 2023-11-24 18:47:12.748119.\n",
      "user spent 111 on test with time 2023-12-04 14:22:36.089256.\n",
      "user spent 112 on test2 with time 2023-12-04 14:25:55.468784.\n",
      "user spent 112 on test3 with time 2023-12-04 14:38:42.999647.\n",
      "\u001b[0m\n",
      "Thought:You are a financial assistance. your job is to answer the user question and Think logicly! You should use tools to help you answer the question. You should perform 2 action at max before deciding the final answer to the question. If the tools dose not return good enough answer, you should answer the question yourself.\n",
      "You have access to the following tools:\n",
      "\n",
      "Google: Use to perfrom Google search through out the internet. But you need to summerise the result yourself\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "get_user_transaction: Use this tool to get user transaction\n",
      "get_latest_user_transaction: Use this tool to get latest user transaction\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Google, Calculator, get_user_transaction, get_latest_user_transaction]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "Question: give me a history of my transaction\n",
      "Thought: I need to use the get_user_transaction tool to get the user's transaction history\n",
      "Action: get_user_transaction\n",
      "Action Input: user_id\n",
      "Observation:\n",
      "Observation: user earn 1000000.0 on shadow with time 2023-11-24 13:14:08.577375.\n",
      "user earn 1000000.0 on shadow with time 2023-11-24 13:17:24.033903.\n",
      "user earn 34.0 on yrt with time 2023-11-24 20:20:52.805577.\n",
      "user earn 200.0 on selling car with time 2023-12-01 14:30:16.060111.\n",
      "user spent 34234 on fried rice with time 2023-11-24 13:19:58.288379.\n",
      "user spent 324 on yes with time 2023-11-24 18:47:12.748119.\n",
      "user spent 111 on test with time 2023-12-04 14:22:36.089256.\n",
      "user spent 112 on test2 with time 2023-12-04 14:25:55.468784.\n",
      "user spent 112 on test3 with time 2023-12-04 14:38:42.999647.\n",
      "\n",
      "Thought: I need to use the get_latest_user_transaction tool to get the latest transaction\n",
      "Action: get_latest_user_transaction\n",
      "Action Input: user_id\n",
      "Observation:\n",
      "\u001b[32;1m\u001b[1;3m I need to use the get_latest_user_transaction tool to get the latest transaction\n",
      "Action: get_latest_user_transaction\n",
      "Action Input: user_id\n",
      "Observation:\u001b[0m2023-12-07 16:34:38,455 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2023-12-07 16:34:38,456 INFO sqlalchemy.engine.Engine SELECT users.id AS users_id, users.name AS users_name, users.age AS users_age \n",
      "FROM users \n",
      "WHERE users.id = ?\n",
      " LIMIT ? OFFSET ?\n",
      "2023-12-07 16:34:38,456 INFO sqlalchemy.engine.Engine [cached since 5.484s ago] ('a8dcc725-2fbc-4544-9eca-845c14b2e93a', 1, 0)\n",
      "2023-12-07 16:34:38,457 INFO sqlalchemy.engine.Engine SELECT income.id AS income_id, income.amount AS income_amount, income.item AS income_item, income.transaction_time AS income_transaction_time, income.user_id AS income_user_id, income.image_path AS income_image_path \n",
      "FROM income \n",
      "WHERE ? = income.user_id\n",
      "2023-12-07 16:34:38,458 INFO sqlalchemy.engine.Engine [cached since 5.482s ago] ('a8dcc725-2fbc-4544-9eca-845c14b2e93a',)\n",
      "2023-12-07 16:34:38,459 INFO sqlalchemy.engine.Engine SELECT expenses.id AS expenses_id, expenses.amount AS expenses_amount, expenses.item AS expenses_item, expenses.transaction_time AS expenses_transaction_time, expenses.user_id AS expenses_user_id, expenses.image_path AS expenses_image_path \n",
      "FROM expenses \n",
      "WHERE ? = expenses.user_id\n",
      "2023-12-07 16:34:38,459 INFO sqlalchemy.engine.Engine [cached since 5.48s ago] ('a8dcc725-2fbc-4544-9eca-845c14b2e93a',)\n",
      "2023-12-07 16:34:38,460 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3muser earn 200.0 on selling car with time 2023-12-01 14:30:16.060111.\n",
      "user spent 112 on test3 with time 2023-12-04 14:38:42.999647.\n",
      "\u001b[0m\n",
      "Thought:You are a financial assistance. your job is to answer the user question and Think logicly! You should use tools to help you answer the question. You should perform 2 action at max before deciding the final answer to the question. If the tools dose not return good enough answer, you should answer the question yourself.\n",
      "You have access to the following tools:\n",
      "\n",
      "Google: Use to perfrom Google search through out the internet. But you need to summerise the result yourself\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "get_user_transaction: Use this tool to get user transaction\n",
      "get_latest_user_transaction: Use this tool to get latest user transaction\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Google, Calculator, get_user_transaction, get_latest_user_transaction]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "Question: give me a history of my transaction\n",
      "Thought: I need to use the get_user_transaction tool to get the user's transaction history\n",
      "Action: get_user_transaction\n",
      "Action Input: user_id\n",
      "Observation:\n",
      "Observation: user earn 1000000.0 on shadow with time 2023-11-24 13:14:08.577375.\n",
      "user earn 1000000.0 on shadow with time 2023-11-24 13:17:24.033903.\n",
      "user earn 34.0 on yrt with time 2023-11-24 20:20:52.805577.\n",
      "user earn 200.0 on selling car with time 2023-12-01 14:30:16.060111.\n",
      "user spent 34234 on fried rice with time 2023-11-24 13:19:58.288379.\n",
      "user spent 324 on yes with time 2023-11-24 18:47:12.748119.\n",
      "user spent 111 on test with time 2023-12-04 14:22:36.089256.\n",
      "user spent 112 on test2 with time 2023-12-04 14:25:55.468784.\n",
      "user spent 112 on test3 with time 2023-12-04 14:38:42.999647.\n",
      "\n",
      "Thought: I need to use the get_latest_user_transaction tool to get the latest transaction\n",
      "Action: get_latest_user_transaction\n",
      "Action Input: user_id\n",
      "Observation:\n",
      "Observation: user earn 200.0 on selling car with time 2023-12-01 14:30:16.060111.\n",
      "user spent 112 on test3 with time 2023-12-04 14:38:42.999647.\n",
      "\n",
      "Thought: I need to use the calculator tool to calculate the total amount of money the user has spent\n",
      "Action: calculator\n",
      "Action Input: 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + \n",
      "\u001b[32;1m\u001b[1;3m I need to use the calculator tool to calculate the total amount of money the user has spent\n",
      "Action: calculator\n",
      "Action Input: 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + \u001b[0m\n",
      "Observation: calculator is not a valid tool, try one of [Google, Calculator, get_user_transaction, get_latest_user_transaction].\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a financial assistance. your job is to answer the user question and Think logicly! You should use tools to help you answer the question. You should perform 2 action at max before deciding the final answer to the question. If the tools dose not return good enough answer, you should answer the question yourself.\n",
      "You have access to the following tools:\n",
      "\n",
      "Google: Use to perfrom Google search through out the internet. But you need to summerise the result yourself\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "get_user_transaction: Use this tool to get user transaction\n",
      "get_latest_user_transaction: Use this tool to get latest user transaction\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Google, Calculator, get_user_transaction, get_latest_user_transaction]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "Question: give me a history of my transaction\n",
      "Thought: I need to use the get_user_transaction tool to get the user's transaction history\n",
      "Action: get_user_transaction\n",
      "Action Input: user_id\n",
      "Observation:\n",
      "Observation: user earn 1000000.0 on shadow with time 2023-11-24 13:14:08.577375.\n",
      "user earn 1000000.0 on shadow with time 2023-11-24 13:17:24.033903.\n",
      "user earn 34.0 on yrt with time 2023-11-24 20:20:52.805577.\n",
      "user earn 200.0 on selling car with time 2023-12-01 14:30:16.060111.\n",
      "user spent 34234 on fried rice with time 2023-11-24 13:19:58.288379.\n",
      "user spent 324 on yes with time 2023-11-24 18:47:12.748119.\n",
      "user spent 111 on test with time 2023-12-04 14:22:36.089256.\n",
      "user spent 112 on test2 with time 2023-12-04 14:25:55.468784.\n",
      "user spent 112 on test3 with time 2023-12-04 14:38:42.999647.\n",
      "\n",
      "Thought: I need to use the get_latest_user_transaction tool to get the latest transaction\n",
      "Action: get_latest_user_transaction\n",
      "Action Input: user_id\n",
      "Observation:\n",
      "Observation: user earn 200.0 on selling car with time 2023-12-01 14:30:16.060111.\n",
      "user spent 112 on test3 with time 2023-12-04 14:38:42.999647.\n",
      "\n",
      "Thought: I need to use the calculator tool to calculate the total amount of money the user has spent\n",
      "Action: calculator\n",
      "Action Input: 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + 324 + 111 + 112 + 112 + 200 + 34234 + \n",
      "Observation: calculator is not a valid tool, try one of [Google, Calculator, get_user_transaction, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsabsri/miniconda3/envs/rnd-project/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 1037, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_latest_user_transaction].\n",
      "Thought: I\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: ` I`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain/agents/agent.py:1066\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(\n\u001b[1;32m   1067\u001b[0m         intermediate_steps,\n\u001b[1;32m   1068\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1069\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m   1070\u001b[0m     )\n\u001b[1;32m   1071\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain/agents/agent.py:636\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_inputs)\n\u001b[0;32m--> 636\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(full_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py:63\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAction\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*:[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]*(.*?)\u001b[39m\u001b[39m\"\u001b[39m, text, re\u001b[39m.\u001b[39mDOTALL):\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     64\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse LLM output: `\u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m         observation\u001b[39m=\u001b[39mMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\n\u001b[1;32m     66\u001b[0m         llm_output\u001b[39m=\u001b[39mtext,\n\u001b[1;32m     67\u001b[0m         send_to_llm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     69\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m re\u001b[39m.\u001b[39msearch(\n\u001b[1;32m     70\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]*Action\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*Input\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*:[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]*(.*)\u001b[39m\u001b[39m\"\u001b[39m, text, re\u001b[39m.\u001b[39mDOTALL\n\u001b[1;32m     71\u001b[0m ):\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: ` I`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/bsabsri/rnd/data/rnd/tax-rnd-ocr/api/chatbot/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m agent_commit({\u001b[39m'\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39mgive me a history of my transaction\u001b[39;49m\u001b[39m\"\u001b[39;49m}) \n",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    307\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain/agents/agent.py:1312\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1312\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m   1313\u001b[0m         name_to_tool_map,\n\u001b[1;32m   1314\u001b[0m         color_mapping,\n\u001b[1;32m   1315\u001b[0m         inputs,\n\u001b[1;32m   1316\u001b[0m         intermediate_steps,\n\u001b[1;32m   1317\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m   1318\u001b[0m     )\n\u001b[1;32m   1319\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1320\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1321\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1322\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain/agents/agent.py:1038\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_next_step\u001b[39m(\n\u001b[1;32m   1030\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[39mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1036\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[39mstr\u001b[39m]]]:\n\u001b[1;32m   1037\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1038\u001b[0m         [\n\u001b[1;32m   1039\u001b[0m             a\n\u001b[1;32m   1040\u001b[0m             \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1041\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1042\u001b[0m                 color_mapping,\n\u001b[1;32m   1043\u001b[0m                 inputs,\n\u001b[1;32m   1044\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1045\u001b[0m                 run_manager,\n\u001b[1;32m   1046\u001b[0m             )\n\u001b[1;32m   1047\u001b[0m         ]\n\u001b[1;32m   1048\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain/agents/agent.py:1038\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_next_step\u001b[39m(\n\u001b[1;32m   1030\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[39mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1036\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[39mstr\u001b[39m]]]:\n\u001b[1;32m   1037\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1038\u001b[0m         [\n\u001b[1;32m   1039\u001b[0m             a\n\u001b[1;32m   1040\u001b[0m             \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1041\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1042\u001b[0m                 color_mapping,\n\u001b[1;32m   1043\u001b[0m                 inputs,\n\u001b[1;32m   1044\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1045\u001b[0m                 run_manager,\n\u001b[1;32m   1046\u001b[0m             )\n\u001b[1;32m   1047\u001b[0m         ]\n\u001b[1;32m   1048\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rnd-project/lib/python3.10/site-packages/langchain/agents/agent.py:1077\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     raise_error \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[39mif\u001b[39;00m raise_error:\n\u001b[0;32m-> 1077\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1078\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn output parsing error occurred. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1079\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1080\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1081\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis is the error: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1082\u001b[0m     )\n\u001b[1;32m   1083\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: ` I`"
     ]
    }
   ],
   "source": [
    "agent_commit({'input':\"give me a history of my transaction\"}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import PromptTemplate\n",
    "# prompt = PromptTemplate(\"\"\"You are a financial assistance. your job is to answer the user question.\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = LLMChain(llm=open_ai,prompt=prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnd-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
